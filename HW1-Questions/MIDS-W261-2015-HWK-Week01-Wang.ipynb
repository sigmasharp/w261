{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.0.0. Define big data. Provide an example of a big data problem in your domain of expertise.\n",
    "\n",
    "Big data is a general term for data (sets) either so large or so complex that conventional data processing applications (RMDB etc) are caught short. In machine learning considerations, this term often focus the use of predictive analytics or certain other advanced methods to extract value or learn from generally complex data sets (in volume, volecity, variety, variability, veracity, visulization and value). (For an adnet company I worked for.) As a 300+MM banner-ad-requests+/day ad network company, we were facing a typical big data problem for all volume, velocity and variety considerations. Since the 'problem' graduately developed through out a 12+ year period of time, we ended up with a more traditional solution (co-lo of clusters of 100+ top-of-the-line servers just for the front end requests with 2 hardware load balancer.) Eventually, we used Hadoop/Pig for the preliminary analyitics. (By the time I left in 2012)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.0.1.In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered.  How would you select a model?\n",
    "\n",
    "First bias is the difference between the expected predicted value and the actual value, or E[y*]−f(x), the variance is the expected squared sum of the difference between the predicted value and the expected predicted value, or E[(y*−E[y*])^2], and the irreducible error (noice) is simply the expected squared sume of the difference between the predicted value and the actual value, or E[(y*−f(x))^2]=E[ε2]=σ^2, which will always be there regardless of the bias and variance pan out. When usig polynomial regression models of degree 1 to 5 for a complex problem dataset, then, the lower the degree (toward 1), the bigger the bias would be, and the smaller the variance [less accurate for tested data but less sensitive to variety of new test data, or underfitting]. On the other hand, the higher the degree (toward 5), the smaller the bias, at the cost of a bigger variance [more accurate for tested data but more sensity to new test data, or overfitting].\n",
    "I would start from the lowest degeree (1) and work my way up to higher degrees. For each situation, I will check the performance (interms of prediction accuracy with the bench mark because this is going to a supervised learning). I'd work my way up until the performance starts to degregate. Then the most previous degree will be the one I'd pick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.1. Read through the provided control script (pNaiveBayes.sh)and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below.  A simple cell in the notebook with a print  statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "### HW1.1. Read through the provided control script (pNaiveBayes.sh)and all of its comments. When you are comfortable with their \n",
    "### purpose and function, respond to the remaining homework questions below.  A simple cell in the notebook with a print \n",
    "### statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)\n",
    "\n",
    "print \"Done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod +x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## author: Alan Wang\n",
    "## Description mapper.py for HW1.2\n",
    "## This mapper will take in a file of m (or less) lines and count occurance \n",
    "## of a single word as its parameter\n",
    "## then it spit out the word counts in '<word> <count>' format when word is found (line by line)\n",
    "\n",
    "#First, the mapper.py which will take a file of n lines in and count the number of occurances of a word from the 'command line'\n",
    "\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # regular express for words\n",
    "\n",
    "if len(sys.argv) != 3:\n",
    "    print \"Usage: mapper.py <file> <word>\"\n",
    "    exit()\n",
    "    \n",
    "fileName = sys.argv[1] #the filename\n",
    "wordList = re.split(\" \", sys.argv[2].lower())  #the wordList for HW1.2 to 1.5\n",
    "\n",
    "forAllWords = False\n",
    "\n",
    "if len(wordList) == 1 and wordList[0] == '*':\n",
    "    forAllWords = True\n",
    "\n",
    "wordCounts = {}\n",
    "\n",
    "with open (fileName, \"r\") as myFile:\n",
    "    for line in myFile:\n",
    "        if forAllWords:\n",
    "            wordList = WORD_RE.findall(line)\n",
    "        quar = line.split(\"\\t\") # potentially a quardruplet containing id-spamflag-subject-email format\n",
    "        if len(quar) != 4 or (quar[1] != \"0\" and quar[1] != '1'): #if there is some problem, ignore\n",
    "            continue\n",
    "            \n",
    "        for word in wordList:\n",
    "            count = WORD_RE.findall(quar[2] + ' ' + quar[3]).count(word)\n",
    "            if count != 0:\n",
    "                if word in wordCounts:\n",
    "                    wordCounts[word] += count\n",
    "                else:\n",
    "                    wordCounts[word] = count\n",
    "\n",
    "if len(wordCounts) != 0:\n",
    "    print '\\n'.join([word + ' ' + str(wordCounts[word]) for word in wordCounts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## author: Alan Wang\n",
    "## Description reducer.py for HW1.2\n",
    "## it reads in an aggregated file (from files generated by the mapper.py), and calculate word counts by words\n",
    "\n",
    "import sys\n",
    "\n",
    "fileNames = sys.argv[1:] #the filename\n",
    "\n",
    "wordCounts = {}\n",
    "\n",
    "for fileName in fileNames:\n",
    "    with open (fileName, \"r\") as myFile:\n",
    "        for line in myFile:\n",
    "            tuple = line.split(' ')\n",
    "            word = tuple[0]\n",
    "            count = tuple[1]\n",
    "            #print word, count\n",
    "\n",
    "            if word in wordCounts:\n",
    "                wordCounts[word] += int(count)\n",
    "            else:\n",
    "                wordCounts[word] = int(count)\n",
    "\n",
    "for word in sorted(wordCounts): \n",
    "    # in real application where the order does not matter, we might want to skip the sorted option\n",
    "    # to avoid the n*ln n performance penalty\n",
    "    print word, wordCounts[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance 10\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "!./pNaiveBayes.sh 4 assistance\n",
    "!tail enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results. To do so, make sure that\n",
    "   \n",
    "* mapper.py and\n",
    "* reducer.py \n",
    "\n",
    "#### that performs a single word Naive Bayes classification. For multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "##### the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "##### NOTE if  “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeld as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimated of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## author: Alan Wang\n",
    "## Description mapper.py for HW1.3\n",
    "## This mapper will take in a file of m (or less) lines and count occurance \n",
    "## of a single word as its parameter\n",
    "## then it spit out the word counts in '<flag> <word> <count>' format when word is found for spam or non spam flag (line by line)\n",
    "\n",
    "#First, the mapper.py which will take a file of n lines in and count the number of occurances of a word from the 'command line'\n",
    "\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # regular express for words\n",
    "\n",
    "if len(sys.argv) != 3:\n",
    "    print \"Usage: mapper.py <file> <word>\"\n",
    "    exit()\n",
    "    \n",
    "fileName = sys.argv[1] #the filename\n",
    "wordList = re.split(\" \", sys.argv[2].lower())  #the wordList for HW1.2 to 1.5\n",
    "\n",
    "forAllWords = False\n",
    "\n",
    "if len(wordList) == 1 and wordList[0] == '*':\n",
    "    forAllWords = True\n",
    "\n",
    "spamWordCounts = {}\n",
    "regWordCounts = {}\n",
    "spamTotal = 0\n",
    "regTotal = 0\n",
    "\n",
    "with open (fileName, \"r\") as myFile:\n",
    "    for line in myFile:\n",
    "        if forAllWords:\n",
    "            wordList = WORD_RE.findall(line)\n",
    "        quar = line.split(\"\\t\") # potentially a quardruplet containing id-spamflag-subject-email format\n",
    "        if len(quar) != 4 or (quar[1] != \"0\" and quar[1] != '1'): #if there is some problem, ignore\n",
    "            continue\n",
    "            \n",
    "        for word in wordList:\n",
    "            subTotal = len(WORD_RE.findall(quar[2] + ' ' + quar[3]))\n",
    "            count = WORD_RE.findall(quar[2] + ' ' + quar[3]).count(word)\n",
    "            \n",
    "            if quar[1] == '1': #spam\n",
    "                spamTotal += subTotal\n",
    "                if count !=0:\n",
    "                    if word in spamWordCounts:\n",
    "                        spamWordCounts[word] += count\n",
    "                    else:\n",
    "                        spamWordCounts[word] = count\n",
    "            else: #regular\n",
    "                regTotal += subTotal\n",
    "                if count !=0:\n",
    "                    if word in regWordCounts:\n",
    "                        regWordCounts[word] += count\n",
    "                    else:\n",
    "                        regWordCounts[word] = count\n",
    "\n",
    "if len(spamWordCounts) != 0:\n",
    "    print '\\n'.join(['1 ' + word + ' ' + str(spamWordCounts[word]) for word in spamWordCounts])\n",
    "    \n",
    "if len(regWordCounts) != 0:\n",
    "    print '\\n'.join(['0 ' + word + ' ' + str(spamWordCounts[word]) for word in spamWordCounts])\n",
    "\n",
    "if spamTotal > 0:\n",
    "    print '1 '+'_total '+str(spamTotal)\n",
    "\n",
    "if regTotal > 0:\n",
    "    print '0 '+'_total '+str(regTotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 44 0.44 0.56\n"
     ]
    }
   ],
   "source": [
    "##Naive Baysean:  ln(p(S|D))/p(~S|D)) = ln(P(S)/P(~S)) + sigma(ln(p(wi|S)/p(wi|~S)))\n",
    "##first, let's find the prior P(S) and P(~S)\n",
    "import sys\n",
    "\n",
    "total = 0\n",
    "spamTotal = 0\n",
    "\n",
    "with open('enronemail_1h.txt', 'r') as dataFile:\n",
    "    for line in dataFile:\n",
    "        quar = line.split(\"\\t\") # potentially a quardruplet containing id-spamflag-subject-email format\n",
    "        if len(quar) != 4 or (quar[1] != \"0\" and quar[1] != '1'): #if there is some problem, ignore, just for format checking\n",
    "            continue\n",
    "        total += 1\n",
    "        if quar[1] == '1':\n",
    "            spamTotal += 1\n",
    "\n",
    "print total, spamTotal, float(spamTotal)/total, (1 - float(spamTotal)/total)\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## author: Alan Wang\n",
    "## Description reducer.py for HW1.3\n",
    "## it reads in an aggregated file (from files generated by the mapper.py), and calculate word counts by words\n",
    "## and yield the spam or no spam prediction based upon naive baysean model\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "fileNames = sys.argv[1:] #the filename\n",
    "\n",
    "spamWordCounts = {}\n",
    "regWordCounts = {}\n",
    "spamTotal = 0\n",
    "regTotal = 0\n",
    "\n",
    "for fileName in fileNames:\n",
    "    with open (fileName, \"r\") as myFile:\n",
    "        for line in myFile:\n",
    "            triplets = line.split(' ')\n",
    "            spam = triplets[0]\n",
    "            word = triplets[1]\n",
    "            count = triplets[2]\n",
    "            #print word, count\n",
    "            \n",
    "            if word == '_total':\n",
    "                if spam == '1':\n",
    "                    spamTotal += int(count)\n",
    "                else: \n",
    "                    regTotal += int(count)\n",
    "            else:\n",
    "                \n",
    "                if spam == '1':\n",
    "                    if word in spamWordCounts:\n",
    "                        spamWordCounts[word] += int(count)\n",
    "                    else:\n",
    "                        spamWordCounts[word] = int(count)\n",
    "                else:\n",
    "                    if word in regWordCounts:\n",
    "                        regWordCounts[word] += int(count)\n",
    "                    else:\n",
    "                        regWordCounts[word] = int(count)\n",
    "\n",
    "#At this point we have all the info to calculate the spam-likelyhood for our source enronemail_1h.txt\n",
    "#  ln(p(S|D))/p(~S|D)) = ln(P(S)/P(~S)) + sigma(ln(p(wi|S)/p(wi|~S)))\n",
    "\n",
    "## From last cell:\n",
    "\n",
    "pS = 0.44 #prior p(S)\n",
    "pR = 0.56 #prior p(~S)\n",
    "pP = math.log(pS/pR) # first term ln(P(S)/P(~S)) the priror ratio\n",
    "\n",
    "testFileName = 'enronemail_1h.txt'\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # regular express for words\n",
    "\n",
    "def probSpam(word):\n",
    "    c = 0\n",
    "    if word in spamWordCounts:\n",
    "        c = spamWordCounts[word]\n",
    "    return float(c+1)/float(spamTotal + len(spamWordCounts)) #smoothing for boundary conditioins like c=0\n",
    "\n",
    "def probReg(word):\n",
    "    c = 0\n",
    "    if word in regWordCounts:\n",
    "        c = regWordCounts[word]\n",
    "    return float(c+1)/float(regTotal + len(regWordCounts)) #smoothing for boundary conditioins like c=0\n",
    "\n",
    "total = 0\n",
    "totalCorrect = 0\n",
    "\n",
    "with open(testFileName, 'r') as testFile:\n",
    "    for line in testFile:\n",
    "        quar = line.split(\"\\t\") # potentially a quardruplet containing id-spamflag-subject-email format\n",
    "        if len(quar) != 4 or (quar[1] != \"0\" and quar[1] != '1'): #if there is some problem, ignore, just for format checking\n",
    "            continue\n",
    "        \n",
    "        total += 1\n",
    "        #otherwise, we perform the 'prediction'\n",
    "        content = WORD_RE.findall(quar[2] + ' ' + quar[3])\n",
    "        \n",
    "        sigma = pP #initalized to pPR\n",
    "        \n",
    "        for word in content: # for each word\n",
    "            if word in spamWordCounts and word in regWordCounts:\n",
    "                ppS = float(spamWordCounts[word])/spamTotal\n",
    "                ppR = float(regWordCounts[word])/regTotal\n",
    "                sigma += math.log(ppS/ppR)\n",
    "        \n",
    "        predict = 1\n",
    "        \n",
    "        if(sigma < 0.0):\n",
    "            predict = 0\n",
    "        \n",
    "        if (sigma >= 0.0 and quar[1] == '1') or (sigma < 0.0 and quar[1] == '0'): # correct\n",
    "            totalCorrect += 1\n",
    "            \n",
    "        print quar[0], quar[1], predict\n",
    "\n",
    "print 'Total # of documents:', total, 'Total correct:', totalCorrect, 'Accuracy Rate:', float(totalCorrect)/total\n",
    "\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0017.1999-12-14.kaminski 0 0\r\n",
      "0017.2000-01-17.beck 0 0\r\n",
      "0017.2001-04-03.williams 0 0\r\n",
      "0017.2003-12-18.GP 1 0\r\n",
      "0017.2004-08-01.BG 1 0\r\n",
      "0017.2004-08-02.BG 1 0\r\n",
      "0018.1999-12-14.kaminski 0 0\r\n",
      "0018.2001-07-13.SA_and_HP 1 1\r\n",
      "0018.2003-12-18.GP 1 1\r\n",
      "Total # of documents: 100 Total correct: 60 Accuracy Rate: 0.6\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "!./pNaiveBayes.sh 4 assistance\n",
    "!tail enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.4. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results\n",
    "#### To do so, make sure that\n",
    "\n",
    "* mapper.py counts all occurrences of a list of words, and\n",
    "* reducer.py \n",
    "\n",
    "#### performs the multiple-word multinomial Naive Bayes classification via the chosen list.  No smoothing is needed in this HW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## author: Alan Wang\n",
    "## Description mapper.py for HW1.4\n",
    "## This mapper will take in a file of m (or less) lines and count occurance \n",
    "## of a single word as its parameter\n",
    "## then it spit out the word counts in '<flag> <word> <count>' format when word is found for spam or non spam flag (line by line)\n",
    "## Note in my version of the mapper and reducer, there is actually no difference between HW1.3 and HW1.4 because I already\n",
    "## started to envision a wordlist, instead of a single word only in HW1.2 and HW1.3, so the program can work on\n",
    "## all cases, including the \"*\" case, which I put at the end.\n",
    "\n",
    "#First, the mapper.py which will take a file of n lines in and count the number of occurances of a word from the 'command line'\n",
    "\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # regular express for words\n",
    "\n",
    "if len(sys.argv) != 3:\n",
    "    print \"Usage: mapper.py <file> <word>\"\n",
    "    exit()\n",
    "    \n",
    "fileName = sys.argv[1] #the filename\n",
    "wordList = re.split(\" \", sys.argv[2].lower())  #the wordList for HW1.2 to 1.5\n",
    "\n",
    "forAllWords = False\n",
    "\n",
    "if len(wordList) == 1 and wordList[0] == '*':\n",
    "    forAllWords = True\n",
    "\n",
    "spamWordCounts = {}\n",
    "regWordCounts = {}\n",
    "spamTotal = 0\n",
    "regTotal = 0\n",
    "\n",
    "with open (fileName, \"r\") as myFile:\n",
    "    for line in myFile:\n",
    "        if forAllWords:\n",
    "            wordList = WORD_RE.findall(line)\n",
    "        quar = line.split(\"\\t\") # potentially a quardruplet containing id-spamflag-subject-email format\n",
    "        if len(quar) != 4 or (quar[1] != \"0\" and quar[1] != '1'): #if there is some problem, ignore\n",
    "            continue\n",
    "            \n",
    "        for word in wordList:\n",
    "            subTotal = len(WORD_RE.findall(quar[2] + ' ' + quar[3]))\n",
    "            count = WORD_RE.findall(quar[2] + ' ' + quar[3]).count(word)\n",
    "            \n",
    "            if quar[1] == '1': #spam\n",
    "                spamTotal += subTotal\n",
    "                if count !=0:\n",
    "                    if word in spamWordCounts:\n",
    "                        spamWordCounts[word] += count\n",
    "                    else:\n",
    "                        spamWordCounts[word] = count\n",
    "            else: #regular\n",
    "                regTotal += subTotal\n",
    "                if count !=0:\n",
    "                    if word in regWordCounts:\n",
    "                        regWordCounts[word] += count\n",
    "                    else:\n",
    "                        regWordCounts[word] = count\n",
    "\n",
    "if len(spamWordCounts) != 0:\n",
    "    print '\\n'.join(['1 ' + word + ' ' + str(spamWordCounts[word]) for word in spamWordCounts])\n",
    "    \n",
    "if len(regWordCounts) != 0:\n",
    "    print '\\n'.join(['0 ' + word + ' ' + str(spamWordCounts[word]) for word in spamWordCounts])\n",
    "\n",
    "if spamTotal > 0:\n",
    "    print '1 '+'_total '+str(spamTotal)\n",
    "\n",
    "if regTotal > 0:\n",
    "    print '0 '+'_total '+str(regTotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## author: Alan Wang\n",
    "## Description reducer.py for HW1.3\n",
    "## it reads in an aggregated file (from files generated by the mapper.py), and calculate word counts by words\n",
    "## and yield the spam or no spam prediction based upon naive baysean model\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "fileNames = sys.argv[1:] #the filename\n",
    "\n",
    "spamWordCounts = {}\n",
    "regWordCounts = {}\n",
    "spamTotal = 0\n",
    "regTotal = 0\n",
    "\n",
    "for fileName in fileNames:\n",
    "    with open (fileName, \"r\") as myFile:\n",
    "        for line in myFile:\n",
    "            triplets = line.split(' ')\n",
    "            spam = triplets[0]\n",
    "            word = triplets[1]\n",
    "            count = triplets[2]\n",
    "            #print word, count\n",
    "            \n",
    "            if word == '_total':\n",
    "                if spam == '1':\n",
    "                    spamTotal += int(count)\n",
    "                else: \n",
    "                    regTotal += int(count)\n",
    "            else:\n",
    "                \n",
    "                if spam == '1':\n",
    "                    if word in spamWordCounts:\n",
    "                        spamWordCounts[word] += int(count)\n",
    "                    else:\n",
    "                        spamWordCounts[word] = int(count)\n",
    "                else:\n",
    "                    if word in regWordCounts:\n",
    "                        regWordCounts[word] += int(count)\n",
    "                    else:\n",
    "                        regWordCounts[word] = int(count)\n",
    "\n",
    "#At this point we have all the info to calculate the spam-likelyhood for our source enronemail_1h.txt\n",
    "#  ln(p(S|D))/p(~S|D)) = ln(P(S)/P(~S)) + sigma(ln(p(wi|S)/p(wi|~S)))\n",
    "\n",
    "## From last cell:\n",
    "\n",
    "pS = 0.44 #prior p(S)\n",
    "pNS = 0.56 #prior p(~S)\n",
    "pPR = math.log(pS/pNS) # first term ln(P(S)/P(~S))\n",
    "\n",
    "testFileName = 'enronemail_1h.txt'\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # regular express for words\n",
    "\n",
    "def probSpam(word):\n",
    "    c = 0\n",
    "    if word in spamWordCounts:\n",
    "        c = spamWordCounts[word]\n",
    "    return float(c+1)/float(spamTotal + len(spamWordCounts)) #smoothing for boundary conditioins like c=0\n",
    "\n",
    "def probReg(word):\n",
    "    c = 0\n",
    "    if word in regWordCounts:\n",
    "        c = regWordCounts[word]\n",
    "    return float(c+1)/float(regTotal + len(regWordCounts)) #smoothing for boundary conditioins like c=0\n",
    "\n",
    "total = 0\n",
    "totalCorrect = 0\n",
    "\n",
    "with open(testFileName, 'r') as testFile:\n",
    "    for line in testFile:\n",
    "        quar = line.split(\"\\t\") # potentially a quardruplet containing id-spamflag-subject-email format\n",
    "        if len(quar) != 4 or (quar[1] != \"0\" and quar[1] != '1'): #if there is some problem, ignore, just for format checking\n",
    "            continue\n",
    "        \n",
    "        total += 1\n",
    "        #otherwise, we perform the 'prediction'\n",
    "        content = WORD_RE.findall(quar[2] + ' ' + quar[3])\n",
    "        \n",
    "        sigma = pPR #initalized to pPR\n",
    "        \n",
    "        for word in content: # for each word\n",
    "            sigma += math.log(probSpam(word)/probReg(word))\n",
    "        \n",
    "        predict = 1\n",
    "        \n",
    "        if(sigma < 0.0):\n",
    "            predict = 0\n",
    "        \n",
    "        if (sigma >= 0.0 and quar[1] == '1') or (sigma < 0.0 and quar[1] == '0'): # correct\n",
    "            totalCorrect += 1\n",
    "            \n",
    "        print quar[0], quar[1], predict\n",
    "\n",
    "print 'Total # of documents:', total, 'Total correct:', totalCorrect, 'Accuracy Rate:', float(totalCorrect)/total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0017.1999-12-14.kaminski 0 0\r\n",
      "0017.2000-01-17.beck 0 0\r\n",
      "0017.2001-04-03.williams 0 0\r\n",
      "0017.2003-12-18.GP 1 0\r\n",
      "0017.2004-08-01.BG 1 0\r\n",
      "0017.2004-08-02.BG 1 0\r\n",
      "0018.1999-12-14.kaminski 0 0\r\n",
      "0018.2001-07-13.SA_and_HP 1 0\r\n",
      "0018.2003-12-18.GP 1 0\r\n",
      "Total # of documents: 100 Total correct: 56 Accuracy Rate: 0.56\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "!./pNaiveBayes.sh 4 \"assistance valium enlargementWithATypo\"\n",
    "!tail enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0017.1999-12-14.kaminski 0 0\r\n",
      "0017.2000-01-17.beck 0 0\r\n",
      "0017.2001-04-03.williams 0 0\r\n",
      "0017.2003-12-18.GP 1 0\r\n",
      "0017.2004-08-01.BG 1 0\r\n",
      "0017.2004-08-02.BG 1 0\r\n",
      "0018.1999-12-14.kaminski 0 0\r\n",
      "0018.2001-07-13.SA_and_HP 1 0\r\n",
      "0018.2003-12-18.GP 1 0\r\n",
      "Total # of documents: 100 Total correct: 56 Accuracy Rate: 0.56\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 *\n",
    "!tail enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
